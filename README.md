# Chatty-AI Ollama service

This is a plugin that enables support for completions by Ollama's built in server, and this is only useful used in conjunction with [chatty-ai](https://github.com/justinhj/chatty-ai.nvim).

## Usage

After installing the chatty-ai system, you should also install [Ollama](https://github.com/ollama) so that you can run local models.

Follow the instructions to install the models you would like to use with chatty-ai as completion services. Once installed you can cofigure an Ollama service and register it with chatty-ai so that it can be used.

In [./sampleconfig/config.lua](./sampleconfig/config.lua) you can find simple instructions to do this.
